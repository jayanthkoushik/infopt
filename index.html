<!DOCTYPE html><html lang="en"><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>Deep Black-Box Optimization with Influence Functions</title><link rel="stylesheet" href="https://jkoushik.me/assets/main.css"><div class="container"><div class="row justify-content-center"><div class="col-12 col-sm-12 col-md-11 col-lg-9 col-xl-8"><h1 id="title"> <small aria-disabled="true" id="theme-switch" class="d-block d-sm-inline text-right float-sm-right mb-n2 mt-n3 my-sm-auto ml-sm-1"></small> Deep Black-Box Optimization with Influence Functions <br> <small></small></h1><p><ul class="lead list-inline"><li class="list-inline-item"> Jayanth Koushik<sup class="sup-right">1</sup><li class="list-inline-item"> Michael J. Tarr<sup class="sup-right">1</sup><li class="list-inline-item"> Aarti Singh<sup class="sup-right">1</sup></ul><ul class="list-inline"><li class="list-inline-item"><sup class="sup-left">1</sup>Carnegie Mellon University</ul><h1>Abstract</h1><p class="lead"> Deep neural networks are increasingly being used to model black-box functions. Examples include modeling brain response to stimuli, material properties under given synthesis conditions, and digital art. In these applications, often the model is a surrogate and the goal is rather to optimize the black-box function to achieve the desired brain response, material property, or digital art characteristics. Moreover, resource constraints imply that, rather than training on a passive dataset, one should focus subsequent sampling on the most informative data points. In the Bayesian setting, this can be achieved by utilizing the ability of Bayesian models such as Gaussian processes to model uncertainty in observed data via posterior variance, which can guide subsequent sampling. However, uncertainty estimates for deep neural networks are largely lacking or are very expensive to compute. For example, bootstrap or cross-validation estimates require re-training the network several times which is often computationally prohibitive. In this work, we use influence functions to estimate the variance of neural network outputs, and design a black-box optimization algorithm similar to confidence bound-based Bayesian algorithms. We demonstrate the effectiveness of our method through experiments on synthetic and real-world optimization problems.<div></div><h1 id="sec:introduction"><span class="header-section-number">1</span> Introduction</h1><p>Black-box optimization, also known as zeroth order optimization, is the problem of finding the global minima or maxima of a function given access to only (possibly noisy) evaluations of the function. Perhaps the most popular black-box optimization approach is in the Bayesian setting, such as Gaussian process (<span class="abbr">GP</span>) optimization, which assumes that the black-box function is sampled from a <abbr title='Gaussian process'>GP</abbr>, and uses an acquisition function such as lower confidence bound (<span class="abbr">LCB</span>) to guide sampling and subsequently update the posterior mean and variance of the <abbr title='Gaussian process'>GP</abbr> model in an iterative manner. However, recently, deep neural networks are increasingly being used to model black-box functions. Examples include modeling brain response to stimuli<span class="citation" data-cites="yamins2014performance agrawal2014pixels kell2018task"><a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a></span>, material properties under given synthesis conditions<span class="citation" data-cites="materialsAL"><a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a></span>, and digital art<span class="citation" data-cites="manovich2015data"><a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a></span>. Often the goal in these problems is optimization of the black-box model, rather than learning the entire model. For example, a human vision researcher might be interested in understanding which images cause maximum activation in a specific brain region<span class="citation" data-cites="ponce2019evolving bashivan2019neural"><a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a></span>; a material scientist is interested in finding optimal experimental conditions that yield a material with desired properties<span class="citation" data-cites="materialsAL"><a href="#fn5" class="footnote-ref" id="fnref5"><sup>5</sup></a></span> or generate digital art with desired characteristics<span class="citation" data-cites="manovich2015data"><a href="#fn6" class="footnote-ref" id="fnref6"><sup>6</sup></a></span>. While a simple approach is to learn a deep model on passively acquired evaluations of the function, and then report its optima, this is wasteful as often the black-box evaluations are expensive (c.f., subject time in a brain scanner is limited, material synthesis experiments are expensive, etc.). Also, often pre-trained models of black-boxes need to be updated subsequently to identify inputs that may lead to novel outputs not explored in training set. For example, in material science a model trained to predict the energy of a pure lattice may need to be updated to understand new low-energy configurations achievable under defects, or deep neural net models of images may need to be updated to achieve desired characteristics of synthetic digital images. Thus, it is of interest to develop sequential optimization methods akin to Bayesian optimization for deep neural networks.<p>Sequential optimization of neural network models requires an acquisition function, similar to Bayesian optimization. However, popular acquisition functions (<abbr title='lower confidence bound'>LCB</abbr>, expectation maximization, Thompson sampling, etc.) are mostly based on an uncertainty measure or confidence bound which characterizes the variability of the predictions. Unfortunately, formal methods for uncertainty quantification that are also computationally feasible for deep neural network models are largely non-existent. For example, bootstrap or cross-validation based estimates of uncertainty require re-training the network several times, which is typically computationally prohibitive. In this paper, we seek to investigate principled and computationally efficient estimators of uncertainty measures (like variance of prediction), that can then be used to guide subsequent sampling for optimization of black-box functions.<p>Specifically, we use the concept of influence functions<span class="citation" data-cites="cook1980characterizations cook1982residuals"><a href="#fn7" class="footnote-ref" id="fnref7"><sup>7</sup></a></span> from classical statistics to approximate the leave-one-out cross-validation estimate of the prediction variance, <em>without having to re-train the model</em>. It is known<span class="citation" data-cites="cook1982residuals"><a href="#fn8" class="footnote-ref" id="fnref8"><sup>8</sup></a></span> that if the loss function is twice-differentiable and strictly convex, then the influence function has a closed-form approximation, and the influence-function based estimate provides an asymptotic approximation of the variance of prediction. Even though the loss function of neural networks is non-differentiable and non-convex, it was recently shown<span class="citation" data-cites="koh2017understanding"><a href="#fn9" class="footnote-ref" id="fnref9"><sup>9</sup></a></span> that in practice, the approximation continues to hold for this case. However, <span class="citation" data-cites="koh2017understanding">ibid.</span> used influence functions to understand the importance of each input on the prediction of a passively trained deep neural network, Influence functions were not investigated for uncertainty quantification and estimation of prediction variance for use in subsequent sampling.<p>A related line of work is activation maximization in neural networks (<span class="abbr">NNs</span>) where the goal is to find input that maximizes the output of a particular unit in the network. However, since the corresponding target functions are known and differentiable, gradient based optimization methods can be used. Still, obtaining results suitable for visualization requires careful tuning and optimization hacks<span class="citation" data-cites="nguyen2016synthesizing"><a href="#fn10" class="footnote-ref" id="fnref10"><sup>10</sup></a></span>. In this paper, we will consider the activation maximization problem in a black-box setting, to mimic neuroscience and material science experiments, where the brain is the black-box function. Furthermore, prior work is passive requiring learning a good model for all inputs, while we focus on collecting new data to sequentially guide the model towards identifying the input which leads to maximum output <em>without necessarily learning a good model for all inputs</em>.<p>There have also been attempts to directly extend Bayesian optimization to neural networks. <span class="citation" data-cites="snoek2015scalable">Snoek et al.<a href="#fn11" class="footnote-ref" id="fnref11"><sup>11</sup></a></span> add a Bayesian linear layer to neural networks, treating the network outputs as basis function. <span class="citation" data-cites="springenberg2016bayesian">Springenberg et al.<a href="#fn12" class="footnote-ref" id="fnref12"><sup>12</sup></a></span> focus on scalability, and use a Monte Carlo approach, combined with scale adaptation. However, our focus is to enable sequential optimization of existing NN models being used in scientific domains. Our contributions can be summarized as follows:<ul><li>We use influence functions to obtain a computationally efficient approximation of prediction variance in neural networks.<li>We propose a computationally efficient method to compute influence functions for neural network predictions. Our approach uses a low-rank approximation of the Hessian, which is represented using an auxillary network, and trained along with the main network.<li>We develop a deep black-box optimization method using these influence function based uncertainty estimates that is valid in the non-Bayesian setting.<li>We demonstrate the efficacy of our method on synthetic and real datasets. Our method can be comparable, and also outperform Bayesian optimization in settings where neural networks may be able to model the underlying function better than GPs.</ul><p>The rest of the paper is organized as follows. In Section <a href="#sec:preliminaries">2</a>, we formally define the problem, and the Bayesian setting we build upon in this work. Our proposed method is described in Section <a href="#sec:method">3</a>, followed by results on synthetic and real datasets in Section <a href="#sec:experiments">4</a>. We conclude with discussion of open problems in Section <a href="#sec:discussion">5</a>.<h1 id="sec:preliminaries"><span class="header-section-number">2</span> Preliminaries</h1><h2 id="sec:problem-setting"><span class="header-section-number">2.1</span> Problem Setting</h2><p>We consider the problem of sequential optimization of a black-box function. Specifically, let <span class="math inline">\(f: \mathcal{X}\to \mathbb{R}\)</span> be a cost function to be minimized. At each step <span class="math inline">\(t\)</span>, we select a point <span class="math inline">\(x_t \in \mathcal{X}\)</span>, and observe a noisy evaluation <span class="math inline">\(y_t = f(x_t) + \epsilon_t\)</span>, where <span class="math inline">\(\epsilon_t\)</span> is independent 0-mean noise. This noisy evaluation is the only way to interact with the function, and we don’t assume any prior knowledge of it. We will use <span class="math inline">\(z\)</span> to denote an input-output pair; <span class="math inline">\(z \equiv (x, y) \in \mathcal{X}\times \mathbb{R}\)</span>.<p>Practical functions in this category (like hyper-parameter optimization for instance) generally tend to be “expensive”, either in terms of time, or resources, or both. This makes it impractical to do a dense “grid search” to identify the minimum; algorithms must use as few evaluations as possible. With a given time budget <span class="math inline">\(T\)</span>, the objective is to minimize the simple regret, <span class="math inline">\(\min_{t=1 \dots T} {f(x_t) - f(x^\ast)}\)</span> where <span class="math inline">\(x^\ast \in \mathop{\mathrm{arg\,min}}_{x \in \mathcal{X}} f(x)\)</span> is a global minimum (not necessarily unique). This measures how close to the optimum an algorithm gets in <span class="math inline">\(T\)</span> steps, and is equivalent to minimizing <span class="math inline">\(\min_{t=1 \dots T} f(x_t)\)</span>.<h2 id="sec:bayesian-optimization"><span class="header-section-number">2.2</span> Bayesian Optimization</h2><p>Bayesian optimization is a popular method for solving black-box optimization problems, which uses <abbr title='Gaussian process'>GP</abbr> models to estimate the unknown cost function. At each step <span class="math inline">\(T\)</span>, newly obtained data <span class="math inline">\((x_T, y_T)\)</span> is used to update a <abbr title='Gaussian process'>GP</abbr> prior, and the posterior distribution is used to define an acquisition function <span class="math inline">\(\alpha_T: \mathcal{X}\to \mathbb{R}\)</span>. The next point to query, <span class="math inline">\(x_{T+1}\)</span> is selected by minimizing the acquisition function; <span class="math inline">\(x_{T+1} = \mathop{\mathrm{arg\,min}}_{x \in \mathcal{X}} \alpha_T(x)\)</span>. Popular acquisition functions are expected improvement (<span class="abbr">EI</span>), maximum probability of improvement (<span class="abbr">MPI</span>), and <abbr title='lower confidence bound'>LCB</abbr>. Here, we will particularly focus on <abbr title='lower confidence bound'>LCB</abbr>, which provides the motivation for our method.<h2 id="sec:gp-lcb"><span class="header-section-number">2.3</span> GP-LCB</h2><p>Consider a <abbr title='Gaussian process'>GP</abbr> model with mean function <span class="math inline">\(0\)</span>, and covariance function <span class="math inline">\(k(\cdot, \cdot)\)</span>. After observing <span class="math inline">\(T\)</span> points, the model is updated to obtain a posterior mean function <span class="math inline">\(\mu_T\)</span>, and a posterior covariance function <span class="math inline">\(k_T(\cdot, \cdot)\)</span>. The <abbr title='lower confidence bound'>LCB</abbr> acquisition function is <span class="math inline">\(\alpha_T^{LCB}(x) = \mu_T(x) - \beta_T^{1/2} \sigma_T(x)\)</span>, where <span class="math inline">\(\sigma_T(x) \equiv k_T(x, x)\)</span>, and <span class="math inline">\(\beta_T\)</span> is a parameter for balancing exploration and exploitation. This expression is easily interpretable; <span class="math inline">\(\mu_T\)</span> is an estimate of expected cost, and <span class="math inline">\(\sigma_T\)</span> is an estimate of uncertainty. The next point is chosen taking both into consideration. Points with low expected cost are exploited, and points with high uncertainty are explored. <span class="math inline">\(\alpha_T\)</span> defines a “pessimistic” estimate (lower confidence bound) for the cost, hence the name of the algorithm.<h1 id="sec:method"><span class="header-section-number">3</span> Method</h1><p>Suppose we have a neural network <span class="math inline">\(g: \mathcal{X}\to \mathbb{R}\)</span> with parameters <span class="math inline">\(\theta \in \Theta\)</span>, trained using a convex loss function <span class="math inline">\(L: (\mathcal{X}\times \mathbb{R}) \times \Theta \to \mathbb{R}^+\)</span>. At a particular step <span class="math inline">\(T\)</span>, we get an estimate of the parameters <span class="math inline">\(\vphantom{\theta}\smash[t]{\hat{\theta}}_T\)</span>, by minimizing <span class="math inline">\((1 / T)\sum\nolimits_{t=1}^T L(z_t, \theta)\)</span>. As noted earlier, <span class="math inline">\(z_t \equiv (x_t, y_t)\)</span>, and we will use <span class="math inline">\(g_\theta\)</span> to denote the network with a particular set of parameters. Now, for any point <span class="math inline">\(x \in \mathcal{X}\)</span>, we have a prediction for the cost function, i.e., <span class="math inline">\(g_{\vphantom{\theta}\smash[t]{\hat{\theta}}_T}(x)\)</span>. So, if we get an estimate <span class="math inline">\(\vphantom{\sigma}\smash[t]{\hat{\sigma}}_T(x)\)</span>, for the variance of <span class="math inline">\(g_{\vphantom{\theta}\smash[t]{\hat{\theta}}_T}(x)\)</span>, we can define an acquisition function <span class="math inline">\(\alpha_T(x) = g_{\vphantom{\theta}\smash[t]{\hat{\theta}}_T}(x) - \vphantom{\sigma}\smash[t]{\hat{\sigma}}_T(x)\)</span>. Then optimization proceeds similar to Bayesian optimization; where we select the next point for querying <span class="math inline">\(x_{T + 1}\)</span> by minimizing <span class="math inline">\(\alpha_T(X)\)</span>. This auxiliary optimization problem is non-convex, but local minima can be obtained through gradient based methods. In practice, it is also common to use multiple restarts when solving this problem, and select the best solution. We now describe our method for estimating the variance using influence functions.<h2 id="sec:influence-functions"><span class="header-section-number">3.1</span> Influence Functions</h2><p>Intuitively, influence functions measure the effect of a small perturbation at a data point on the parameters of a model. We up-weight a particular point <span class="math inline">\(z^+\)</span> from the training set <span class="math inline">\(\left\{z_t\right\}_{t=1}^T\)</span>, and obtain a new set of parameters <span class="math inline">\(\vphantom{\theta}\smash[t]{\hat{\theta}}_T^+(z^+, \nu)\)</span> by minimizing the reweighted loss function, <span class="math inline">\((1 / T)\sum\nolimits_{t=1}^T L(z_t, \theta) + \nu L(z^+, \theta)\)</span>. We define the influence of <span class="math inline">\(z^+\)</span> on <span class="math inline">\(\vphantom{\theta}\smash[t]{\hat{\theta}}_T\)</span> as the change <span class="math inline">\(\vphantom{\theta}\smash[t]{\hat{\theta}}_T^+(z^+, \nu) - \vphantom{\theta}\smash[t]{\hat{\theta}}_T\)</span> caused by an adding an infinitesimal weight to <span class="math inline">\(z^+\)</span>. Formally, <span class="math display">\[ \mathcal{I}_{\vphantom{\theta}\smash[t]{\hat{\theta}}_T}(z^+) = \lim_{\nu \to 0} \frac{\vphantom{\theta}\smash[t]{\hat{\theta}}_T^+(z^+, \nu) - \vphantom{\theta}\smash[t]{\hat{\theta}}_T}{\nu} = \frac{\partial{\vphantom{\theta}\smash[t]{\hat{\theta}}_T^+(z^+, \nu)}}{\partial{\nu}}. \]</span><p>Importantly, the influence function can be approximated using the following result. <span class="math display">\[ \mathcal{I}_{\vphantom{\theta}\smash[t]{\hat{\theta}}_T}(z^+) \approx -H_{\vphantom{\theta}\smash[t]{\hat{\theta}}_T}^{-1} \nabla_\theta L(z^+, \vphantom{\theta}\smash[t]{\hat{\theta}}_T), \]</span><p>where <span class="math inline">\(\nabla_\theta L(z^+, \vphantom{\theta}\smash[t]{\hat{\theta}}_T)\)</span> is the gradient of the loss with respected to the parameters evaluated at <span class="math inline">\((z^+, \vphantom{\theta}\smash[t]{\hat{\theta}}_T)\)</span>, and <span class="math inline">\(H_{\vphantom{\theta}\smash[t]{\hat{\theta}}_T} \equiv (1 / T)\sum\nolimits_{t=1}^T \nabla_\theta^2 L(z_t, \vphantom{\theta}\smash[t]{\hat{\theta}}_T)\)</span> is the Hessian. Now, we can use the chain rule to extend this approximation to the influence on the prediction of <span class="math inline">\(g_{\vphantom{\theta}\smash[t]{\hat{\theta}}_T}\)</span>. For a test point <span class="math inline">\(x^\dagger\)</span>, let <span class="math inline">\(\mathcal{I}_{g_{\vphantom{\theta}\smash[t]{\hat{\theta}}_T}}(x^\dagger, z^+)\)</span> be the influence of <span class="math inline">\(z^+\)</span> on <span class="math inline">\(g_{\vphantom{\theta}\smash[t]{\hat{\theta}}_T}(x^\dagger)\)</span>. So,<p><span class="math display">\[ \begin{aligned} \mathcal{I}_{g_{\vphantom{\theta}\smash[t]{\hat{\theta}}_T}}(x^\dagger, z^+) &amp;= \frac{\partial{g_{\vphantom{\theta}\smash[t]{\hat{\theta}}_T^+(z^+, \nu)}(x^\dagger)}}{\partial{\nu}} \\ &amp;= \frac{\partial{g_{\vphantom{\theta}\smash[t]{\hat{\theta}}_T}(x^\dagger)}}{\partial{\theta}} \frac{\partial{\vphantom{\theta}\smash[t]{\hat{\theta}}_T^+(z^+, \nu)}}{\partial{\nu}} \\ &amp;= \frac{\partial{g_{\vphantom{\theta}\smash[t]{\hat{\theta}}_T}(x^\dagger)}}{\partial{\theta}} \mathcal{I}_{\vphantom{\theta}\smash[t]{\hat{\theta}}_T}(z^+) \\ &amp;\approx -\frac{\partial{g_{\vphantom{\theta}\smash[t]{\hat{\theta}}_T}(x^\dagger)}}{\partial{\theta}} H_{\vphantom{\theta}\smash[t]{\hat{\theta}}_T}^{-1} \nabla_\theta L(z^+, \vphantom{\theta}\smash[t]{\hat{\theta}}_T). \end{aligned} \]</span><h2 id="sec:variance-estimation"><span class="header-section-number">3.2</span> Variance Estimation</h2><p>Finally, we estimate the variance by computing the average squared influence over the training points. <span class="math display">\[ \vphantom{\sigma}\smash[t]{\hat{\sigma}}_T(x) = \frac{1}{T}\sum\limits_{t=1}^T \mathcal{I}_{g_{\vphantom{\theta}\smash[t]{\hat{\theta}}_T}}(x, z_t)^2. \]</span> In semi-parametric theory, influence is formalized through the behavior of asymptotically linear estimators, and under regularity conditions, it can be shown that the average squared influence converges to the asymptotic variance<span class="citation" data-cites="tsiatis2007semiparametric"><a href="#fn13" class="footnote-ref" id="fnref13"><sup>13</sup></a></span>.<h2 id="sec:implementation"><span class="header-section-number">3.3</span> Implementation</h2><p>The procedure described above cannot be directly applied to neural networks since the Hessian is not positive-definite in the general case. We address this issue by making a low-rank approximation, <span class="math inline">\(H_{\vphantom{\theta}\smash[t]{\hat{\theta}}} \approx Q \equiv PP^T\)</span>. Let <span class="math inline">\(P = U \Sigma V^T\)</span> be a singular value decomposition (<span class="abbr">SVD</span>) of <span class="math inline">\(P\)</span>. Then, <span class="math inline">\(Q^\dagger \equiv U \Sigma^{\dagger^2} U^T\)</span> is the Moore-Penrose pseudoinverse of <span class="math inline">\(Q\)</span>, where <span class="math inline">\(\Sigma^{\dagger^2}\)</span> is a diagonal matrix with reciprocals of the squared non-zero singular values. With this, for any vector <span class="math inline">\(v\)</span> we can approximate the product with the inverse Hessian. <span class="math display">\[ H_{\vphantom{\theta}\smash[t]{\hat{\theta}}}^\dagger v \approx Q^\dagger v = U \Sigma^{\dagger^2} U^T v. \]</span> We represent the low-rank approximation using a second neural network with a single hidden layer. The network uses shared weights similar to an autoencoder, and given input <span class="math inline">\(v\)</span>, computes <span class="math inline">\(PP^Tv\)</span>. We train this network to approximate <span class="math inline">\(H_{\vphantom{\theta}\smash[t]{\hat{\theta}}} \nabla_\theta L(z, \vphantom{\theta}\smash[t]{\hat{\theta}})\)</span>, using samples from the training data. The Hessian vector product can be computed efficiently by performing two backward passes through the network (Perlmutter’s method). After updating the network at each step <span class="math inline">\(T\)</span>, the <abbr title='singular value decomposition'>SVD</abbr> of <span class="math inline">\(P\)</span> is computed, which allows efficient computation of <span class="math inline">\(H_{\vphantom{\theta}\smash[t]{\hat{\theta}}_T}^{-1} \nabla_\theta L(z^+, \vphantom{\theta}\smash[t]{\hat{\theta}}_T)\)</span>. The full algorithm (<abbr title='NN-INF'>NN-INF</abbr>) is shown in Figure <a href="#fig:algorithm">1</a>.<figure> <img src="fig/alg.png" alt="Figure 1: Algorithm" id="fig:algorithm" /><figcaption>Figure 1: Algorithm</figcaption></figure><h2 id="sec:gp-inf"><span class="header-section-number">3.4</span> GP-INF</h2><p>The influence approximation for variance can also be applied to <abbr title='Gaussian process'>GP</abbr> models, by viewing them as performing kernel ridge regression. In this case, there is a closed form expression for the influence<span class="citation" data-cites="ollerer2015influence"><a href="#fn14" class="footnote-ref" id="fnref14"><sup>14</sup></a></span>, so we can directly compute variance approximation. This gives a method similar to <abbr title='GP-LCB'>GP-LCB</abbr>, where we use the influence approximation of variance instead of the posterior variance. We term this method <abbr title='GP-INF'>GP-INF</abbr>, and use it as an additional baseline in our experiments.<h1 id="sec:experiments"><span class="header-section-number">4</span> Experiments</h1><h2 id="sec:synthetic-function-maximization"><span class="header-section-number">4.1</span> Synthetic function maximization</h2><p>First, we compare our method with <abbr title='Gaussian process'>GP</abbr> based algorithms using common test functions used in optimization: five dimensional Ackley function<span class="citation" data-cites="ackley2012connectionist"><a href="#fn15" class="footnote-ref" id="fnref15"><sup>15</sup></a></span>, and ten dimensional Rastrigin function<span class="citation" data-cites="rastrigin1974systems"><a href="#fn16" class="footnote-ref" id="fnref16"><sup>16</sup></a></span>. For the Ackley function, we use a network with 3 hidden layers, with 8, 8, and 4 hidden units respectively. And for the Rastrigin function, we again use a network with 3 hidden layers, but with 16, 16, and 8 hidden units. In both cases, we approximate the Hessian with a rank 5 matrix. We report two sets of results, using different schemes for setting the <span class="math inline">\(\beta_t\)</span> parameter (used in <abbr title='INF'>INF</abbr> and <abbr title='lower confidence bound'>LCB</abbr> methods).<p>Figure <a href="#fig:synth">2</a> (1) shows the instantaneous regret over 500 iterations with <span class="math inline">\(\beta_t = c\sqrt{t}\log^{2}(10t)\)</span> (based on the theoretical results presented by <span class="citation" data-cites="srinivas2009gaussian">Srinivas et al.<a href="#fn17" class="footnote-ref" id="fnref17"><sup>17</sup></a></span>). We set <span class="math inline">\(c=0.1\)</span> for <abbr title='Gaussian process'>GP</abbr> methods, and <span class="math inline">\(c=0.01\)</span> for <abbr title='NN-INF'>NN-INF</abbr>. We did not find <span class="math inline">\(c\)</span> to have a significant effect on performance, but for consistency, we used scaled <span class="math inline">\(\beta_t\)</span> for <abbr title='NN-INF'>NN-INF</abbr> by 10 in all cases. Figure <a href="#fig:synth">2</a> (2) shows the same results, but with <span class="math inline">\(\beta_t\)</span> held constant throughout the experiment. We have <span class="math inline">\(\beta_t = 2\)</span> for <abbr title='Gaussian process'>GP</abbr> methods, and <span class="math inline">\(\beta_t = 0.2\)</span> for <abbr title='NN-INF'>NN-INF</abbr>.<figure> <img src="fig/synth.png" alt="Figure 2: Optimization of synthetic functions" id="fig:synth" /><figcaption>Figure 2: Optimization of synthetic functions</figcaption></figure><h2 id="sec:neural-network-output-maximization"><span class="header-section-number">4.2</span> Neural network output maximization</h2><p>We now demonstrate results on a task inspired from neuroscience. To understand the properties of a neuron, brain region etc., experimenters collect response signals (neuron firing rate, increase in blood oxygenation etc.) to different stimuli in order to identify maximally activating inputs. This is generally done in an ad-hoc manner, whereby experimenters hand pick, or manually create a restricted set of images designed to address a given theoretical question. This can lead to biased results caused by insufficient exploration of the input space. One way to address this issue is to perform adaptive stimulus selection over the full input space.<p>To simulate the setting of stimulus selection in neuroscience, we first trained a convolutional neural network (<span class="abbr">CNN</span>) to classify images from the <abbr title='MNIST'>MNIST</abbr> dataset. The output layer of this <abbr title='convolutional neural network'>CNN</abbr> has 10 units, each corresponding to one of the <abbr title='MNIST'>MNIST</abbr> digits (0 to 9). Given an input image, the output of each unit is proportional to the probability (as predicted by the model), that the image belongs to the particular class. With this, we can define an optimization task: find the image that maximizes the output of a particular unit. This is similar to a neuroscience visual stimulus selection experiment, where the output unit could be a single neuron in the visual cortex.<p>Given the difficultly of this optimization problem, it is important to exploit available prior knowledge. For a visual experiment, this could be in the form of a pre-trained network. Here, we pre-train our <abbr title='convolutional neural network'>CNN</abbr> model for binary classification of two digits different from the target digit; for example (classifying ‘5’ vs. ‘6’ when the target digit is ‘2’. For the model, we use a smaller <abbr title='convolutional neural network'>CNN</abbr> than the target; with two convolution layers, each with a single filter. Figure <a href="#fig:mnist">3</a> shows the target neuron output for two different settings. In Figure <a href="#fig:mnist">3</a> (a), the target digit is ‘2’, and the <abbr title='convolutional neural network'>CNN</abbr> is pre-trained for classifying ‘5’ vs. ‘6’. In Figure <a href="#fig:mnist">3</a> (b), the target digit is ‘3’, and the <abbr title='convolutional neural network'>CNN</abbr> is pre-trained for classifying ‘1’ vs. ‘8’. In both cases, we see that the <abbr title='convolutional neural network'>CNN</abbr> model is able to exploit the prior information, and achieve better performance compared to the <abbr title='GP-LCB'>GP-LCB</abbr> baseline. This is a promising result showing the feasibility of large scale adaptive sitmulus selection.<figure> <img src="fig/mnist.png" alt="Figure 3: MNIST" id="fig:mnist" /><figcaption>Figure 3: MNIST</figcaption></figure><h1 id="sec:discussion"><span class="header-section-number">5</span> Discussion</h1><p>In this paper, we use the notion of influence functions from classical statistics to estimate the variance of prediction made using a neural network model, without having to retrain the model on multiple subsets of data as in bootstrap or cross-validation based estimates. We additionally use these uncertainty estimates to design a deep black-box optimization algorithm, that can be used to optimize a black-box function such as brain response or desired material property with sequentially collected data. We show the efficacy of our algorithm on synthetic and real datasets.<p>There are several directions for future work. First, the uncertainty estimates we propose are backed by theoretical underpinning under convexity assumptions when the samples are assumed to be independent and it is of interest to develop theoretical guarantees for the non-convex and sequentially dependent samples setting which arises in optimization. The latter should be possible given parallel analysis in the Bayesian setting. Such non-Bayesian confidence bounds that are valid for sequential data can then also be used for active learning of black-box functions or deep models. Second, while the method does not require retraining the NN model at each iteration for variance estimation, the model does require retraining as new data is collected. While this is inevitable in optimization and active learning settings,the computational complexity can be improved by not training the model to convergence at each iteration. For example, in,<span class="citation" data-cites="Awasthi:2017:PLE:3038256.3006384"><a href="#fn18" class="footnote-ref" id="fnref18"><sup>18</sup></a></span> and references therein, computational efficiency is achieved for active learning of linear separators by training the model to lower accuracy initially (e.g. it should be matched to the lower statistical accuracy due to limited samples initially) and then increasing the computational accuracy at subsequent iterations. Finally, we have only explored the notion of uncertainty (coupled with prediction maximization) to guide subsequent sampling. However, since neural networks learn a feature representation, another way to guide sampling is via the notion of expressiveness (c.f.<span class="citation" data-cites="sener2017active"><a href="#fn19" class="footnote-ref" id="fnref19"><sup>19</sup></a></span>) that selects data points which help improve the learnt feature representation. It is interesting to compare and potentially combine the notions of uncertainty and expressiveness to guide sampling for optimization as well as active learning of black-box functions modeled via deep neural networks.<div id="refs" class="references"><div id="ref-ackley2012connectionist"><p>Ackley, David. <em>A Connectionist Machine for Genetic Hillclimbing</em>. Vol. 28. Springer Science &amp; Business Media, 2012.</div><div id="ref-agrawal2014pixels"><p>Agrawal, Pulkit, Dustin Stansbury, Jitendra Malik, and Jack L Gallant. “Pixels to Voxels: Modeling Visual Representation in the Human Brain.” <em>arXiv Preprint arXiv:1407.5104</em>, 2014.</div><div id="ref-Awasthi:2017:PLE:3038256.3006384"><p>Awasthi, Pranjal, Maria Florina Balcan, and Philip M. Long. “The Power of Localization for Efficiently Learning Linear Separators with Noise.” <em>J. ACM</em> 63, no. 6 (January 2017): 50:1–50:27. <a href="https://doi.org/10.1145/3006384" class="uri">https://doi.org/10.1145/3006384</a>.</div><div id="ref-bashivan2019neural"><p>Bashivan, Pouya, Kohitij Kar, and James J DiCarlo. “Neural Population Control via Deep Image Synthesis.” <em>Science</em> 364, no. 6439 (2019): eaav9436.</div><div id="ref-cook1980characterizations"><p>Cook, R Dennis, and Sanford Weisberg. “Characterizations of an Empirical Influence Function for Detecting Influential Cases in Regression.” <em>Technometrics</em> 22, no. 4 (1980): 495–508.</div><div id="ref-cook1982residuals"><p>———. <em>Residuals and Influence in Regression</em>. New York: Chapman; Hall, 1982.</div><div id="ref-kell2018task"><p>Kell, Alexander JE, Daniel LK Yamins, Erica N Shook, Sam V Norman-Haignere, and Josh H McDermott. “A Task-Optimized Neural Network Replicates Human Auditory Behavior, Predicts Brain Responses, and Reveals a Cortical Processing Hierarchy.” <em>Neuron</em> 98, no. 3 (2018): 630–44.</div><div id="ref-koh2017understanding"><p>Koh, Pang Wei, and Percy Liang. “Understanding Black-Box Predictions via Influence Functions.” In <em>Proceedings of the 34th International Conference on Machine Learning-Volume 70</em>, 1885–94. JMLR. org, 2017.</div><div id="ref-manovich2015data"><p>Manovich, Lev. “Data Science and Digital Art History.” <em>International Journal for Digital Art History</em>, no. 1 (2015).</div><div id="ref-nguyen2016synthesizing"><p>Nguyen, Anh, Alexey Dosovitskiy, Jason Yosinski, Thomas Brox, and Jeff Clune. “Synthesizing the Preferred Inputs for Neurons in Neural Networks via Deep Generator Networks.” In <em>Advances in Neural Information Processing Systems</em>, 3387–95, 2016.</div><div id="ref-ollerer2015influence"><p>Öllerer, Viktoria, Christophe Croux, and Andreas Alfons. “The Influence Function of Penalized Regression Estimators.” <em>Statistics</em> 49, no. 4 (2015): 741–65.</div><div id="ref-ponce2019evolving"><p>Ponce, Carlos R, Will Xiao, Peter Schade, Till S Hartmann, Gabriel Kreiman, and Margaret S Livingstone. “Evolving Super Stimuli for Real Neurons Using Deep Generative Networks.” <em>bioRxiv</em>, 2019, 516484.</div><div id="ref-rastrigin1974systems"><p>Rastrigin, LA. “Systems of Extremal Control.” <em>Nauka</em>, 1974.</div><div id="ref-sener2017active"><p>Sener, Ozan, and Silvio Savarese. “Active Learning for Convolutional Neural Networks: A Core-Set Approach.” <em>arXiv Preprint arXiv:1708.00489</em>, 2017.</div><div id="ref-snoek2015scalable"><p>Snoek, Jasper, Oren Rippel, Kevin Swersky, Ryan Kiros, Nadathur Satish, Narayanan Sundaram, Mostofa Patwary, Mr Prabhat, and Ryan Adams. “Scalable Bayesian Optimization Using Deep Neural Networks.” In <em>International Conference on Machine Learning</em>, 2171–80, 2015.</div><div id="ref-springenberg2016bayesian"><p>Springenberg, Jost Tobias, Aaron Klein, Stefan Falkner, and Frank Hutter. “Bayesian Optimization with Robust Bayesian Neural Networks.” In <em>Advances in Neural Information Processing Systems</em>, 4134–42, 2016.</div><div id="ref-srinivas2009gaussian"><p>Srinivas, Niranjan, Andreas Krause, Sham M Kakade, and Matthias Seeger. “Gaussian Process Optimization in the Bandit Setting: No Regret and Experimental Design.” <em>arXiv Preprint arXiv:0912.3995</em>, 2009.</div><div id="ref-tsiatis2007semiparametric"><p>Tsiatis, Anastasios. <em>Semiparametric Theory and Missing Data</em>. Springer Science &amp; Business Media, 2007.</div><div id="ref-materialsAL"><p>Xue, D., P. V. Balachandran, J. Hogden, J. Theiler, and T. Lookman. “Accelerated Search for Materials with Targeted Properties by Adaptive Design.” <em>Nature Communications</em> 7 (2016): 11241.</div><div id="ref-yamins2014performance"><p>Yamins, Daniel LK, Ha Hong, Charles F Cadieu, Ethan A Solomon, Darren Seibert, and James J DiCarlo. “Performance-Optimized Hierarchical Models Predict Neural Responses in Higher Visual Cortex.” <em>Proceedings of the National Academy of Sciences</em> 111, no. 23 (2014): 8619–24.</div></div><section class="footnotes"><hr /><ol><li id="fn1"><p>Yamins et al., “Performance-Optimized Hierarchical Models Predict Neural Responses in Higher Visual Cortex,” <em>Proceedings of the National Academy of Sciences</em> 111, no. 23 (2014): 8619–24; Agrawal et al., “Pixels to Voxels: Modeling Visual Representation in the Human Brain,” <em>arXiv Preprint arXiv:1407.5104</em>, 2014; Kell et al., “A Task-Optimized Neural Network Replicates Human Auditory Behavior, Predicts Brain Responses, and Reveals a Cortical Processing Hierarchy,” <em>Neuron</em> 98, no. 3 (2018): 630–44.<a href="#fnref1" class="footnote-back">↩</a><li id="fn2"><p>Xue et al., “Accelerated Search for Materials with Targeted Properties by Adaptive Design,” <em>Nature Communications</em> 7 (2016): 11241.<a href="#fnref2" class="footnote-back">↩</a><li id="fn3"><p>Manovich, “Data Science and Digital Art History,” <em>International Journal for Digital Art History</em>, no. 1 (2015).<a href="#fnref3" class="footnote-back">↩</a><li id="fn4"><p>Ponce et al., “Evolving Super Stimuli for Real Neurons Using Deep Generative Networks,” <em>bioRxiv</em>, 2019, 516484; Bashivan et al., “Neural Population Control via Deep Image Synthesis,” <em>Science</em> 364, no. 6439 (2019): eaav9436.<a href="#fnref4" class="footnote-back">↩</a><li id="fn5"><p>Xue et al., “Accelerated Search for Materials with Targeted Properties by Adaptive Design,” <em>Nature Communications</em> 7 (2016): 11241.<a href="#fnref5" class="footnote-back">↩</a><li id="fn6"><p>Manovich, “Data Science and Digital Art History,” <em>International Journal for Digital Art History</em>, no. 1 (2015).<a href="#fnref6" class="footnote-back">↩</a><li id="fn7"><p>Cook and Weisberg, “Characterizations of an Empirical Influence Function for Detecting Influential Cases in Regression,” <em>Technometrics</em> 22, no. 4 (1980): 495–508; Cook and Weisberg, <em>Residuals and Influence in Regression</em> (New York: Chapman; Hall, 1982).<a href="#fnref7" class="footnote-back">↩</a><li id="fn8"><p>Cook and Weisberg, <em>Residuals and Influence in Regression</em> (New York: Chapman; Hall, 1982).<a href="#fnref8" class="footnote-back">↩</a><li id="fn9"><p>Koh and Liang, “Understanding Black-Box Predictions via Influence Functions,” in <em>Proceedings of the 34th International Conference on Machine Learning-Volume 70</em> (JMLR. org, 2017), 1885–94.<a href="#fnref9" class="footnote-back">↩</a><li id="fn10"><p>Nguyen et al., “Synthesizing the Preferred Inputs for Neurons in Neural Networks via Deep Generator Networks,” in <em>Advances in Neural Information Processing Systems</em>, 2016, 3387–95.<a href="#fnref10" class="footnote-back">↩</a><li id="fn11"><p>“Scalable Bayesian Optimization Using Deep Neural Networks,” in <em>International Conference on Machine Learning</em>, 2015, 2171–80.<a href="#fnref11" class="footnote-back">↩</a><li id="fn12"><p>“Bayesian Optimization with Robust Bayesian Neural Networks,” in <em>Advances in Neural Information Processing Systems</em>, 2016, 4134–42.<a href="#fnref12" class="footnote-back">↩</a><li id="fn13"><p>Tsiatis, <em>Semiparametric Theory and Missing Data</em> (Springer Science &amp; Business Media, 2007).<a href="#fnref13" class="footnote-back">↩</a><li id="fn14"><p>Öllerer et al., “The Influence Function of Penalized Regression Estimators,” <em>Statistics</em> 49, no. 4 (2015): 741–65.<a href="#fnref14" class="footnote-back">↩</a><li id="fn15"><p>Ackley, <em>A Connectionist Machine for Genetic Hillclimbing</em>, vol. 28 (Springer Science &amp; Business Media, 2012).<a href="#fnref15" class="footnote-back">↩</a><li id="fn16"><p>Rastrigin, “Systems of Extremal Control,” <em>Nauka</em>, 1974.<a href="#fnref16" class="footnote-back">↩</a><li id="fn17"><p>“Gaussian Process Optimization in the Bandit Setting: No Regret and Experimental Design,” <em>arXiv Preprint arXiv:0912.3995</em>, 2009.<a href="#fnref17" class="footnote-back">↩</a><li id="fn18"><p>Awasthi et al., “The Power of Localization for Efficiently Learning Linear Separators with Noise,” <em>J. ACM</em> 63, no. 6 (January 2017): 50:1–50:27, <a href="https://doi.org/10.1145/3006384" class="uri">https://doi.org/10.1145/3006384</a>.<a href="#fnref18" class="footnote-back">↩</a><li id="fn19"><p>Sener and Savarese, “Active Learning for Convolutional Neural Networks: A Core-Set Approach,” <em>arXiv Preprint arXiv:1708.00489</em>, 2017.<a href="#fnref19" class="footnote-back">↩</a></ol></section></div></div></div><script src="https://jkoushik.me/assets/main.js"></script>
